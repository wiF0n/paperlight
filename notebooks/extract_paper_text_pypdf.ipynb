{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutLMv3: Pre-training for Document AI\n",
      "with Unified Text and Image Masking\n",
      "Yupan Huang∗\n",
      "Sun Yat-sen University\n",
      "huangyp28@mail2.sysu.edu.cnTengchao Lv\n",
      "Microsoft Research Asia\n",
      "tengchaolv@microsoft.comLei Cui\n",
      "Microsoft Research Asia\n",
      "lecu@microsoft.com\n",
      "Yutong Lu\n",
      "Sun Yat-sen University\n",
      "luyutong@mail.sysu.edu.cnFuru Wei\n",
      "Microsoft Research Asia\n",
      "fuwei@microsoft.com\n",
      "ABSTRACT\n",
      "Self-supervised pre-training techniques have achieved remarkable\n",
      "progress in Document AI. Most multimodal pre-trained models\n",
      "use a masked language modeling objective to learn bidirectional\n",
      "representations on the text modality, but they differ in pre-training\n",
      "objectives for the image modality. This discrepancy adds difficulty\n",
      "to multimodal representation learning. In this paper, we propose\n",
      "LayoutLMv3 to pre-train multimodal Transformers for Document\n",
      "AI with unified text and image masking. Additionally, LayoutLMv3\n",
      "is pre-trained with a word-patch alignment objective to learn cross-\n",
      "modal alignment by predicting whether the corresponding image\n",
      "patch of a text word is masked. The simple unified architecture\n",
      "and training objectives make LayoutLMv3 a general-purpose pre-\n",
      "trained model for both text-centric and image-centric Document AI\n",
      "tasks. Experimental results show that LayoutLMv3 achieves state-\n",
      "of-the-art performance not only in text-centric tasks, including\n",
      "form understanding, receipt understanding, and document visual\n",
      "question answering, but also in image-centric tasks such as docu-\n",
      "ment image classification and document layout analysis. The code\n",
      "and models are publicly available at https://aka.ms/layoutlmv3.\n",
      "CCS CONCEPTS\n",
      "•Applied computing →Document analysis ;•Computing\n",
      "methodologies→Natural language processing .\n",
      "KEYWORDS\n",
      "document ai, layoutlm, multimodal pre-training, vision-and-language\n",
      "ACM Reference Format:\n",
      "Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. Lay-\n",
      "outLMv3: Pre-training for Document AI with Unified Text and Image Mask-\n",
      "ing. In Proceedings of the 30th ACM International Conference on Multimedia\n",
      "(MM ’22), October 10–14, 2022, Lisboa, Portugal. ACM, New York, NY, USA,\n",
      "10 pages. https://doi.org/10.1145/3503161.3548112\n",
      "∗Contribution during internship at Microsoft Research. Corresponding authors: Lei\n",
      "Cui and Furu Wei.\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "MM ’22, October 10–14, 2022, Lisboa, Portugal\n",
      "©2022 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-9203-7/22/10. . . $15.00\n",
      "https://doi.org/10.1145/3503161.3548112\n",
      "(a) Text-centric form under-\n",
      "standing on FUNSD\n",
      "(b) Image-centric layout anal-\n",
      "ysis on PubLayNet\n",
      "Figure 1: Examples of Document AI Tasks.\n",
      "1 INTRODUCTION\n",
      "In recent years, pre-training techniques have been making waves\n",
      "in the Document AI community by achieving remarkable progress\n",
      "on document understanding tasks [ 2,13–15,17,25,28,31,32,40,\n",
      "41,50,52,54–56]. As shown in Figure 1, a pre-trained Document\n",
      "AI model can parse layout and extract key information for various\n",
      "documents such as scanned forms and academic papers, which is\n",
      "important for industrial applications and academic research [8].\n",
      "Self-supervised pre-training techniques have made rapid progress\n",
      "in representation learning due to their successful applications of re-\n",
      "constructive pre-training objectives. In NLP research, BERT firstly\n",
      "proposed “masked language modeling” (MLM) to learn bidirec-\n",
      "tional representations by predicting the original vocabulary id of\n",
      "a randomly masked word token based on its context [9]. Whereas\n",
      "most performant multimodal pre-trained Document AI models use\n",
      "the MLM proposed by BERT for text modality, they differ in pre-\n",
      "training objectives for image modality as depicted in Figure 2. For\n",
      "example, DocFormer learns to reconstruct image pixels through a\n",
      "CNN decoder [ 2], which tends to learn noisy details rather than\n",
      "high-level structures such as document layouts [ 43,45]. SelfDoc\n",
      "proposes to regress masked region features [ 31], which is noisier\n",
      "and harder to learn than classifying discrete features in a smaller\n",
      "vocabulary [ 6,18]. The different granularities of image (e.g., dense\n",
      "image pixels or contiguous region features) and text (i.e., discretearXiv:2204.08387v3  [cs.CL]  19 Jul 2022\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(\"../papers/2204.08387v3.LayoutLMv3__Pre_training_for_Document_AI_with_Unified_Text_and_Image_Masking.pdf\")\n",
    "page = reader.pages[0]\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rotated text discovered. Output will be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          LayoutLMv3:Pre-trainingforDocumentAI\n",
      "                                 withUnifiedTextandImageMasking\n",
      "\n",
      "                Yupan Huang∗                                          Tengchao Lv                                             Lei Cui\n",
      "             Sun Yat-sen University                             Microsoft Research Asia                             Microsoft Research Asia\n",
      "        huangyp28@mail2.sysu.edu.cn                           tengchaolv@microsoft.com                                lecu@microsoft.com\n",
      "                                             Yutong Lu                                            Furu Wei\n",
      "                                       Sun Yat-sen University                             Microsoft Research Asia\n",
      "                                    luyutong@mail.sysu.edu.cn                              fuwei@microsoft.com\n",
      "ABSTRACT\n",
      "Self-supervised pre-training techniques have achieved remarkable\n",
      "progress in Document AI. Most multimodal pre-trained models\n",
      "use a masked language modeling objective to learn bidirectional\n",
      "representationsonthetextmodality,buttheydifferinpre-training\n",
      "objectives for the image modality. This discrepancy adds difficulty\n",
      "tomultimodalrepresentationlearning.Inthispaper,wepropose\n",
      "LayoutLMv3topre-trainmultimodalTransformersforDocument\n",
      "AIwithunifiedtextandimagemasking.Additionally,LayoutLMv3\n",
      "ispre-trainedwithaword-patchalignmentobjectivetolearncross-\n",
      "modalalignmentbypredictingwhetherthecorrespondingimage\n",
      "patch of a text word is masked. The simple unified architecture\n",
      "andtrainingobjectivesmakeLayoutLMv3ageneral-purposepre-\n",
      "trainedmodelforbothtext-centricandimage-centricDocumentAI                           (a)  Text-centric  form  under-         (b) Image-centric layout anal-\n",
      "tasks. Experimental resultsshow that LayoutLMv3 achievesstate-                      standingonFUNSD                          ysisonPubLayNet\n",
      "of-the-art performance not only in text-centric tasks, including\n",
      "formunderstanding, receiptunderstanding,and documentvisual                                    Figure1:ExamplesofDocumentAITasks.\n",
      "questionanswering,butalso inimage-centrictaskssuchasdocu-\n",
      "mentimage classification anddocumentlayoutanalysis. Thecode\n",
      "and models are publicly available at https://aka.ms/layoutlmv3.\n",
      "CCSCONCEPTS                                                                        1   INTRODUCTION\n",
      "                                                                                   Inrecent years,pre-training techniques havebeenmaking waves\n",
      "• Applied computing→ Document analysis; • Computing                                in theDocument AIcommunity byachievingremarkable progress\n",
      "methodologies→ Natural language processing.                                        on document understanding tasks [2, 13– 15, 17, 25, 28, 31, 32, 40,\n",
      "KEYWORDS                                                                           41, 50, 52, 54– 56]. As shown in Figure 1, a pre-trained Document\n",
      "                                                                                   AImodelcanparse layoutandextractkeyinformationforvarious\n",
      "documentai,layoutlm,multimodalpre-training,vision-and-language                     documentssuchasscannedformsandacademicpapers,whichis\n",
      "ACMReferenceFormat:                                                                important for industrial applications and academic research [8].\n",
      "Yupan Huang, Tengchao Lv, Lei Cui, YutongLu, and Furu Wei. 2022. Lay-                 Self-supervisedpre-trainingtechniqueshavemaderapidprogress\n",
      "outLMv3:Pre-trainingforDocumentAIwithUnifiedTextandImageMask-                      inrepresentationlearningduetotheirsuccessfulapplicationsofre-\n",
      "ing.In Proceedings of the 30th ACM International Conference on Multimedia          constructive pre-training objectives. In NLP research, BERT firstly\n",
      "(MM ’22), October 10–14, 2022, Lisboa, Portugal.  ACM,NewYork,NY,USA,              proposed “masked language modeling” (MLM) to learn bidirec-\n",
      "10 pages. https://doi.org/10.1145/3503161.3548112                                  tionalrepresentations bypredictingthe originalvocabularyidof\n",
      "\n",
      "∗ContributionduringinternshipatMicrosoftResearch.Correspondingauthors:Lei          arandomly maskedword tokenbased onitscontext [9].Whereas\n",
      "Cui and Furu Wei.                                                                  mostperformantmultimodalpre-trainedDocumentAImodelsuse\n",
      "                                                                                   the MLM proposed by BERT for text modality, they differ in pre-\n",
      "Permission to make digital or hard copies of all or part of this work for personal ortrainingobjectivesforimagemodalityasdepictedinFigure2.For\n",
      "classroom use isgranted without fee providedthat copies are notmade or distributed\n",
      "forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation        example, DocFormer learns to reconstruct image pixels through a\n",
      "onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM               CNN decoder [2], which tends to learn noisy details rather than\n",
      "mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,        high-level structures such as document layouts [43, 45]. SelfDoc\n",
      "topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora\n",
      "fee. Request permissions from permissions@acm.org.                                 proposes to regress masked region features [31], which is noisier\n",
      "MM ’22, October 10–14, 2022, Lisboa, Portugal                                      and harder to learn than classifying discrete features in a smaller\n",
      "© 2022 Association for Computing Machinery.                                        vocabulary [6, 18]. The different granularities of image (e.g., dense\n",
      "ACM ISBN 978-1-4503-9203-7/22/10...$15.00\n",
      "https://doi.org/10.1145/3503161.3548112                                            imagepixels orcontiguousregion features)andtext (i.e.,discrete\n"
     ]
    }
   ],
   "source": [
    "print(page.extract_text(extraction_mode=\"layout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperlight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
