{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import arxiv\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_community.utilities import TextRequestsWrapper\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_results(query):\n",
    "    search = GoogleSearchAPIWrapper()\n",
    "    return search.results(query, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=top_3_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LayoutLMv3: Pre-training for Document AI with Unified Text and ...',\n",
       "  'link': 'https://arxiv.org/abs/2204.08387',\n",
       "  'snippet': 'Apr 18, 2022 ... Title:LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking ... Abstract:Self-supervised pre-training techniques have\\xa0...'},\n",
       " {'title': 'Semantic Table Detection with LayoutLMv3',\n",
       "  'link': 'https://arxiv.org/abs/2211.15504',\n",
       "  'snippet': 'Nov 25, 2022 ... Title:Semantic Table Detection with LayoutLMv3 ... Abstract:This paper presents an application of the LayoutLMv3 model for semantic table\\xa0...'},\n",
       " {'title': 'DocILE Benchmark for Document Information Localization and ...',\n",
       "  'link': 'https://arxiv.org/abs/2302.05658',\n",
       "  'snippet': 'Feb 11, 2023 ... The benchmark comes with several baselines, including RoBERTa, LayoutLMv3 and DETR-based Table Transformer; applied to both tasks of the\\xa0...'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searches = tool.run(\"layoutlmv3\")\n",
    "searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/abs/2204.08387',\n",
       " 'https://arxiv.org/abs/2211.15504',\n",
       " 'https://arxiv.org/abs/2302.05658']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [search['link'] for search in searches]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = TextRequestsWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = [requests.get(link) for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_list[0], \"html.parser\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(property='og:title')[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose \\\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at \\\\url{https://aka.ms/layoutlmv3}.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('meta', property='og:description')[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract document index from link\n",
    "def extract_arxiv_id(link):\n",
    "    return link.split('/')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate arxiv client\n",
    "arxiv_client = arxiv.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_search = arxiv.Search(id_list=[extract_arxiv_id(links[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper = next(arxiv_client.results(arxiv_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download file into temp directory\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    arxiv_paper.download_source(dirpath=temp_dir, filename=\"paper.tar.gz\")\n",
    "\n",
    "    # read the file\n",
    "    with tarfile.open(f\"{temp_dir}/paper.tar.gz\", \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith(\"arxiv.tex\"):\n",
    "                file = tar.extractfile(member)\n",
    "                file_content = file.read().decode(\"utf-8\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%%\\n%% This is file `sample-sigconf.tex\\',\\n%% generated with the docstrip utility.\\n%%\\n%% The original source files were:\\n%%\\n%% samples.dtx  (with options: `sigconf\\')\\n%% \\n%% IMPORTANT NOTICE:\\n%% \\n%% For the copyright see the source file.\\n%% \\n%% Any modified versions of this file must be renamed\\n%% with new filenames distinct from sample-sigconf.tex.\\n%% \\n%% For distribution of the original source see the terms\\n%% for copying and modification in the file samples.dtx.\\n%% \\n%% This generated file may be distributed as long as the\\n%% original source files, as listed above, are part of the\\n%% same distribution. (The sources need not necessarily be\\n%% in the same archive or directory.)\\n%%\\n%% Commands for TeXCount\\n%TC:macro \\\\cite [option:text,text]\\n%TC:macro \\\\citep [option:text,text]\\n%TC:macro \\\\citet [option:text,text]\\n%TC:envir table 0 1\\n%TC:envir table* 0 1\\n%TC:envir tabular [ignore] word\\n%TC:envir displaymath 0 word\\n%TC:envir math 0 word\\n%TC:envir comment 0 0\\n%%\\n%%\\n%% The first command in your LaTeX source must be the \\\\documentclass command.\\n% \\\\documentclass[sigconf,anonymous,review]{acmart}\\n\\\\documentclass[sigconf]{acmart}\\n%% NOTE that a single column version may be required for \\n%% submission and peer review. This can be done by changing\\n%% the \\\\doucmentclass[...]{acmart} in this template to \\n%% \\\\documentclass[manuscript,screen]{acmart}\\n%% \\n%% To ensure 100% compatibility, please check the white list of\\n%% approved LaTeX packages to be used with the Master Article Template at\\n%% https://www.acm.org/publications/taps/whitelist-of-latex-packages \\n%% before creating your document. The white list page provides \\n%% information on how to submit additional LaTeX packages for \\n%% review and adoption.\\n%% Fonts used in the template cannot be substituted; margin \\n%% adjustments are not allowed.\\n%%\\n\\n\\\\usepackage{hyperref}\\n\\\\usepackage{url}\\n\\n\\n\\\\usepackage{graphicx}\\n\\\\usepackage{subcaption}\\n\\\\usepackage{multirow}\\n\\\\usepackage{multicol}\\n\\\\usepackage{booktabs}\\n%%\\n\\n\\\\newcommand{\\\\p}[1]{\\\\textrm{Pr}( #1 )}  % probability\\n\\\\newcommand{\\\\R}{\\\\mathbb{R}}   % real numbers\\n\\\\newcommand{\\\\card}[1]{\\\\left\\\\vert{#1}\\\\right\\\\vert}  % size of set\\n\\\\newcommand{\\\\bg}{\\\\textsc{bg}}  % background label\\n\\\\newcommand{\\\\term}[1]{\\\\emph{#1}}  % defining a new term\\n\\\\newcommand{\\\\vect}[1]{\\\\mathbf{#1}}   % vector var\\n\\\\newcommand{\\\\mat}[1]{\\\\mathbf{#1}}    % matrix var\\n\\\\newcommand{\\\\appropto}{\\\\mathrel{\\\\vcenter{\\n  \\\\offinterlineskip\\\\halign{\\\\hfil$##$\\\\cr\\n    \\\\propto\\\\cr\\\\noalign{\\\\kern2pt}\\\\sim\\\\cr\\\\noalign{\\\\kern-2pt}}}}}\\n    \\n\\\\definecolor{demphcolor}{RGB}{144,144,144}\\n\\\\newcommand{\\\\demph}[1]{\\\\textcolor{demphcolor}{#1}}\\n\\n%% \\\\BibTeX command to typeset BibTeX logo in the docs\\n\\\\AtBeginDocument{%\\n  \\\\providecommand\\\\BibTeX{{%\\n    \\\\normalfont B\\\\kern-0.5em{\\\\scshape i\\\\kern-0.25em b}\\\\kern-0.8em\\\\TeX}}}\\n\\n%% Rights management information.  This information is sent to you\\n%% when you complete the rights form.  These commands have SAMPLE\\n%% values in them; it is your responsibility as an author to replace\\n%% the commands and values with those provided to you when you\\n%% complete the rights form.\\n\\n\\\\copyrightyear{2022}\\n\\\\acmYear{2022}\\n\\\\setcopyright{acmcopyright}\\\\acmConference[MM \\'22]{Proceedings of the 30th ACM International Conference on Multimedia}{October 10--14, 2022}{Lisboa, Portugal}\\n\\\\acmBooktitle{Proceedings of the 30th ACM International Conference on Multimedia (MM \\'22), October 10--14, 2022, Lisboa, Portugal}\\n\\\\acmPrice{15.00}\\n\\\\acmDOI{10.1145/3503161.3548112}\\n\\\\acmISBN{978-1-4503-9203-7/22/10}\\n\\n\\n%%\\n%% end of the preamble, start of the body of the document source.\\n\\\\begin{document}\\n\\\\fancyhead{}\\n% remove the headers from the top of each page per the preparation instructions, as these are not needed and will be updated with the chairs\\' actual session names during the pagination/indexing process\\n\\n%%\\n%% The \"title\" command has an optional parameter,\\n%% allowing the author to define a \"short title\" to be used in page headers.\\n\\\\title[LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking]{LayoutLMv3: Pre-training for Document AI \\\\\\\\ with Unified Text and Image Masking}\\n\\n%%\\n%% The \"author\" command and its associated commands are used to define\\n%% the authors and their affiliations.\\n%% Of note is the shared affiliation of the first two authors, and the\\n%% \"authornote\" and \"authornotemark\" commands\\n%% used to denote shared contribution to the research.\\n\\n\\\\author{Yupan Huang}\\n\\\\authornote{Contribution during internship at Microsoft Research. Corresponding authors: Lei Cui and Furu Wei.}\\n% \\\\authornote{This work was done while Yupan Huang was visiting Microsoft Research Asia as a research intern.}\\n\\\\affiliation{\\\\institution{Sun Yat-sen University}\\n\\\\country{}}\\n\\\\email{huangyp28@mail2.sysu.edu.cn}\\n\\n\\\\author{Tengchao Lv}\\n\\\\affiliation{\\\\institution{Microsoft Research Asia}\\n\\\\country{}}\\n\\\\email{tengchaolv@microsoft.com}\\n\\n\\\\author{Lei Cui}\\n\\\\affiliation{\\\\institution{Microsoft Research Asia}\\n\\\\country{}}\\n\\\\email{lecu@microsoft.com}\\n\\n\\\\author{Yutong Lu}\\n\\\\affiliation{\\\\institution{Sun Yat-sen University}\\n\\\\country{}}\\n\\\\email{luyutong@mail.sysu.edu.cn}\\n\\n\\\\author{Furu Wei}\\n\\\\affiliation{\\\\institution{Microsoft Research Asia}\\n\\\\country{}}\\n\\\\email{fuwei@microsoft.com}\\n%%\\n%% By default, the full list of authors will be used in the page\\n%% headers. Often, this list is too long, and will overlap\\n%% other information printed in the page headers. This command allows\\n%% the author to define a more concise list\\n%% of authors\\' names for this purpose.\\n% \\\\renewcommand{\\\\shortauthors}{Trovato and Tobin, et al.}\\n\\\\renewcommand{\\\\shortauthors}{Huang, et al.}\\n\\n%%%%%%%%% ABSTRACT\\n\\\\begin{abstract}\\nSelf-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose \\\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at \\\\url{https://aka.ms/layoutlmv3}.\\n\\\\end{abstract}\\n\\n%%\\n%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.\\n%% Please copy and paste the code instead of the example below.\\n%%\\n\\\\begin{CCSXML}\\n<ccs2012>\\n<concept>\\n<concept_id>10010405.10010497.10010504.10010505</concept_id>\\n<concept_desc>Applied computing~Document analysis</concept_desc>\\n<concept_significance>500</concept_significance>\\n</concept>\\n<concept>\\n<concept_id>10010147.10010178.10010179</concept_id>\\n<concept_desc>Computing methodologies~Natural language processing</concept_desc>\\n<concept_significance>300</concept_significance>\\n</concept>\\n</ccs2012>\\n\\\\end{CCSXML}\\n\\n\\\\ccsdesc[500]{Applied computing~Document analysis}\\n\\\\ccsdesc[300]{Computing methodologies~Natural language processing}\\n\\n%%\\n%% Keywords. The author(s) should pick words that accurately describe\\n%% the work being presented. Separate the keywords with commas.\\n\\\\keywords{document ai, layoutlm, multimodal pre-training, vision-and-language}\\n\\n\\n%%\\n%% This command processes the author and affiliation and title\\n%% information and builds the first part of the formatted document.\\n\\\\maketitle\\n\\n%%%%%%%%% BODY TEXT\\n\\n\\\\begin{figure}[t]\\n\\\\centering\\n    \\\\begin{subfigure}[b]{0.45\\\\linewidth}\\n        \\\\fbox{\\\\includegraphics[width=\\\\linewidth]{figures/funsd.png}}\\n        \\\\caption{Text-centric form understanding on FUNSD}\\n        \\\\label{fig:1a}\\n    \\\\end{subfigure}\\n    \\\\qquad\\n    % \\\\quad\\n    %add desired spacing between images, e. g. ~, \\\\quad, \\\\qquad, \\\\hfill etc. \\n    \\\\begin{subfigure}[b]{0.44\\\\linewidth}\\n        \\\\fbox{\\\\includegraphics[width=0.9\\\\linewidth]{figures/dla.png}}\\n        \\\\caption{Image-centric layout analysis on PubLayNet}\\n        \\\\label{fig:1b}\\n    \\\\end{subfigure}\\n    \\\\caption{Examples of Document AI Tasks.}\\n    \\\\label{fig:1}\\n\\\\end{figure} \\n\\n\\\\begin{figure}[t]\\n    \\\\centering\\n    \\\\includegraphics[width=1\\\\linewidth]{figures/intro.pdf}\\n    \\\\caption{\\\\textbf{Comparisons with existing works} (e.g., DocFormer \\\\protect\\\\cite{Appalaraju_2021_ICCV} and SelfDoc \\\\protect\\\\cite{li2021selfdoc}) on\\n    \\\\textbf{(1) image embedding}: our LayoutLMv3 uses linear patches to reduce the computational bottleneck of CNNs and eliminate the need for region supervision in training object detectors;\\n    \\\\textbf{(2) pre-training objectives on image modality}: our LayoutLMv3 learns to reconstruct discrete image tokens of masked patches instead of raw pixels or region features to capture high-level layout structures rather than noisy details.\\n    } \\\\label{fig:intro}\\n\\\\end{figure}\\n\\n\\\\section{Introduction}\\n\\nIn recent years, pre-training techniques have been making waves in the Document AI community by achieving remarkable progress on document understanding tasks~\\\\cite{xu2020layoutlm,xu-etal-2021-layoutlmv2,xu2021layoutxlm,pramanik2020towards,garncarek2021lambert,hong2022bros,Powalski2021GoingFB,wu2021lampret,Li2021StructuralLMSP,li2021selfdoc,Appalaraju_2021_ICCV,li2021structext,gu2021unidoc,wang2022LiLT,gu2022xylayoutlm,lee2022formnet}. As shown in Figure~\\\\ref{fig:1}, a pre-trained Document AI model can parse layout and extract key information for various documents such as scanned forms and academic papers, which is important for industrial applications and academic research~\\\\cite{cui2021document}.\\n\\nSelf-supervised pre-training techniques have made rapid progress in representation learning due to their successful applications of reconstructive pre-training objectives.\\nIn NLP research, BERT firstly proposed ``masked language modeling\\'\\' (MLM) to learn bidirectional representations by predicting the original vocabulary id of a randomly masked word token based on its context~\\\\cite{devlin2019bert}.\\nWhereas most performant multimodal pre-trained Document AI models use the MLM proposed by BERT for text modality, they differ in pre-training objectives for image modality as depicted in Figure~\\\\ref{fig:intro}.\\nFor example, DocFormer learns to reconstruct image pixels through a CNN decoder~\\\\cite{Appalaraju_2021_ICCV}, which tends to learn noisy details rather than high-level structures such as document layouts~\\\\cite{salimans2017pixelcnn++,ramesh2021zero}.\\nSelfDoc proposes to regress masked region features~\\\\cite{li2021selfdoc}, which is noisier and harder to learn than classifying discrete features in a smaller vocabulary~\\\\cite{cho2020x,huang2021unifying}.\\nThe different granularities of image (e.g., dense image pixels or contiguous region features) and text (i.e., discrete tokens) objectives further add difficulty to cross-modal alignment learning, which is essential to multimodal representation learning.\\n\\nTo overcome the discrepancy in pre-training objectives of text and image modalities and facilitate multimodal representation learning, we propose \\\\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with unified text and image masking objectives MLM and MIM.\\nAs shown in Figure~\\\\ref{fig:architecture}, LayoutLMv3 learns to reconstruct masked word tokens of the text modality and symmetrically reconstruct masked patch tokens of the image modality.\\nInspired by DALL-E~\\\\cite{ramesh2021zero} and BEiT~\\\\cite{bao2022beit}, we obtain the target image tokens from latent codes of a discrete VAE.\\nFor documents, each text word corresponds to an image patch. To learn this cross-modal alignment, we propose a Word-Patch Alignment (WPA) objective to predict whether the corresponding image patch of a text word is masked.\\n\\nInspired by ViT~\\\\cite{dosovitskiy2020vit} and ViLT~\\\\cite{kim2021vilt}, LayoutLMv3 directly leverages raw image patches from document images without complex pre-processing steps such as page object detection.\\nLayoutLMv3 jointly learns image, text and multimodal representations in a Transformer model with unified MLM, MIM and WPA objectives.\\nThis makes LayoutLMv3 the first multimodal pre-trained Document AI model without CNNs for image embeddings, which significantly saves parameters and gets rid of region annotations.\\nThe simple unified architecture and objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric tasks and image-centric Document AI tasks.\\n\\n\\nWe evaluated pre-trained LayoutLMv3 models across five public benchmarks, including text-centric benchmarks: FUNSD~\\\\cite{jaume2019funsd} for form understanding, CORD~\\\\cite{park2019cord} for receipt understanding, DocVQA~\\\\cite{mathew2021docvqa} for document visual question answering, and image-centric benchmarks: RVL-CDIP~\\\\cite{harley2015icdar} for document image classification, PubLayNet~\\\\cite{zhong2019publaynet} for document layout analysis. \\nExperiment results demonstrate that LayoutLMv3 achieves state-of-the-art performance on these benchmarks with parameter efficiency. \\nFurthermore, LayoutLMv3 is easy to reproduce for its simple and neat architecture and pre-training objectives.\\n\\n\\nOur contributions are summarized as follows:\\n\\\\begin{itemize}\\n\\t\\\\item LayoutLMv3 is the first multimodal model in Document AI that does not rely on a pre-trained CNN or Faster R-CNN backbone to extract visual features, which significantly saves parameters and eliminates region annotations.\\n\\t\\\\item LayoutLMv3 mitigates the discrepancy between text and image multimodal representation learning with unified discrete token reconstructive objectives MLM and MIM. We further propose a Word-Patch Alignment (WPA) objective to facilitate cross-modal alignment learning.\\n\\t\\\\item LayoutLMv3 is a general-purpose model for both text-centric and image-centric Document AI tasks. For the first time, we demonstrate the generality of multimodal Transformers to vision tasks in Document AI.\\n\\t\\\\item Experimental results show that LayoutLMv3 achieves state-of-the-art performance in text-centric tasks and image-centric tasks in Document AI. The code and models are publicly available at \\\\url{https://aka.ms/layoutlmv3}.\\n\\\\end{itemize}\\n\\n\\n\\\\begin{figure*}[t]\\n    \\\\centering\\n    \\\\includegraphics[width=0.9\\\\linewidth]{figures/architecture.pdf}\\n    \\\\caption{\\n    \\\\textbf{The architecture and pre-training objectives of LayoutLMv3.}\\n    LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking objectives.\\n    Given an input document image and its corresponding text and layout position information, the model takes the linear projection of patches and word tokens as inputs and encodes them into contextualized vector representations.\\n    LayoutLMv3 is pre-trained with discrete token reconstructive objectives of Masked Language Modeling (MLM) and Masked Image Modeling (MIM).\\n    Additionally, LayoutLMv3 is pre-trained with a Word-Patch Alignment (WPA) objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. ``Seg\\'\\' denotes segment-level positions. ``[CLS]\\'\\', ``[MASK]\\'\\', ``[SEP]\\'\\' and ``[SPE]\\'\\' are special tokens.\\n    } \\\\label{fig:architecture}\\n\\\\end{figure*}\\n\\n\\n\\n\\\\section{LayoutLMv3}\\nFigure~\\\\ref{fig:architecture} gives an overview of the LayoutLMv3.\\n\\n\\\\subsection{Model Architecture}\\nLayoutLMv3 applies a unified text-image multimodal Transformer to learn cross-modal representations.\\nThe Transformer has a multi-layer architecture and each layer mainly consists of multi-head self-attention and \\nposition-wise fully connected \\nfeed-forward networks~\\\\cite{vaswani2017attention}.\\nThe input of Transformer is a concatenation of text embedding $\\\\mat{Y}=\\\\vect{y}_{1:L}$ and image embedding $\\\\mat{X}=\\\\vect{x}_{1:M}$ sequences, where $L$ and $M$ are sequence lengths for text and image respectively.\\nThrough the Transformer, the last layer outputs text-and-image contextual representations.\\n\\n\\\\noindent \\\\textbf{Text Embedding.}\\nText embedding is a combination of word embeddings and position embeddings.\\nWe pre-processed document images with an off-the-shelf OCR toolkit to obtain textual content and corresponding 2D position information.\\nWe initialize \\\\textit{the word embeddings} with a word embedding matrix from a pre-trained model RoBERTa~\\\\cite{liu2019roberta}.\\n\\\\textit{The position embeddings} include 1D position and 2D layout position embeddings, where the 1D position refers to the index of tokens within the text sequence, and the \\\\textbf{2D layout position} refers to the bounding box coordinates of the text sequence.\\nFollowing the LayoutLM, we normalize all coordinates by the size of images, and use embedding layers to embed x-axis, y-axis, width and height features separately~\\\\cite{xu2020layoutlm}.\\nThe LayoutLM and LayoutLMv2 adopt word-level layout positions, where each word has its positions.\\nInstead, we adopt segment-level layout positions that words in a segment share the same 2D position since the words usually express the same semantic meaning~\\\\cite{Li2021StructuralLMSP}.\\n\\n\\\\noindent \\\\textbf{Image Embedding.}\\nExisting multimodal models in Document AI either extract CNN grid features~\\\\cite{xu-etal-2021-layoutlmv2,Appalaraju_2021_ICCV} or rely on an object detector like Faster R-CNN~\\\\cite{Ren2015FasterRT} to extract region features~\\\\cite{xu2020layoutlm,Powalski2021GoingFB,li2021selfdoc,gu2021unidoc} for image embeddings, which accounts for heavy computation bottleneck or require region supervision.\\nInspired by ViT~\\\\cite{dosovitskiy2020vit} and ViLT~\\\\cite{kim2021vilt}, we represent document images with linear projection features of image patches before feeding them into the multimodal Transformer.\\nSpecifically, we resize a document image into $H\\\\times W$ and denote the image with $\\\\vect{I} \\\\in \\\\R^{C \\\\times H \\\\times W}$, where $C$, $H$ and $W$ are the channel size, width and height of the image respectively.\\nWe then split the image into a sequence of uniform $P\\\\times P$ patches, linearly project the image patches to $D$ dimensions and flatten them into a sequence of vectors, which length is $M={HW}/{P^2}$.\\nThen we add learnable 1D position embeddings to each patch since we have not observed improvements from using 2D position embeddings in our preliminary experiments.\\nLayoutLMv3 is the first multimodal model in Document AI that does not rely on CNNs to extract image features, which is vital to Document AI models to reduce parameters or remove complex pre-processing steps.\\n\\nWe insert semantic 1D relative position and spatial 2D relative position as bias terms in self-attention networks for text and image modalities following LayoutLMv2\\\\cite{xu-etal-2021-layoutlmv2}.\\n\\n\\n\\n\\n\\\\subsection{Pre-training Objectives}\\nLayoutLMv3 is pre-trained with the MLM, MIM, and WPA objectives to learn multimodal representation in a self-supervised learning manner.\\nFull pre-training objectives of LayoutLMv3 is defined as $L = L_{MLM} + L_{MIM} + L_{WPA}$.\\n\\n\\n\\\\noindent \\\\textbf{Objective I: Masked Language Modeling (MLM).}\\nFor the language side, our MLM is inspired by the masked language modeling in BERT~\\\\cite{devlin2019bert} and masked visual-language modeling in LayoutLM~\\\\cite{xu2020layoutlm} and LayoutLMv2~\\\\cite{xu-etal-2021-layoutlmv2}.\\nWe mask 30\\\\% of text tokens with a span masking strategy with span lengths drawn from a Poisson distribution ($\\\\lambda=3$)~\\\\cite{lewis2020bart,joshi2020spanbert}.\\nThe pre-training objective is to maximize the log-likelihood of the correct masked text tokens $\\\\vect{y}_l$ based on the contextual representations of corrupted sequences of image tokens $\\\\mat{X}^{M\\'}$ and text tokens $\\\\mat{Y}^{L\\'}$, where $M\\'$ and $L\\'$ represent the masked positions.\\nWe denote parameters of the Transformer model with $\\\\theta$ and minimize the subsequent cross-entropy loss:\\n\\\\begin{align}\\nL_{MLM}\\\\left(\\\\theta\\\\right) &= -\\\\sum_{l=1}^{L\\'} \\\\log p_{\\\\theta}\\\\left(\\\\vect{y}_\\\\ell \\\\mid \\\\mat{X}^{M\\'}, \\\\mat{Y}^{L\\'}\\\\right)\\n\\\\end{align}\\nAs we keep the layout information unchanged, this objective facilitates the model to learn the correspondence between layout information and text and image context.\\n\\n\\n\\\\noindent \\\\textbf{Objective II: Masked Image Modeling (MIM).}\\nTo encourage the model to interpret visual content from contextual text and image representations, we adapt the MIM pre-training objective in BEiT~\\\\cite{bao2022beit} to our multimodal Transformer model.\\nThe MIM objective is a symmetry to the MLM objective, that we randomly mask a percentage of about 40\\\\% image tokens with the blockwise masking strategy~\\\\cite{bao2022beit}.\\nThe MIM objective is driven by a cross-entropy loss to reconstruct the masked image tokens $\\\\vect{x}_m$ under the context of their surrounding text and image tokens.\\n\\\\begin{align}\\nL_{MIM}\\\\left(\\\\theta\\\\right) &= -\\\\sum_{m=1}^{M\\'}\\\\log p_{\\\\theta}\\\\left(\\\\vect{x}_m \\\\mid \\\\mat{X}^{M\\'}, \\\\mat{Y}^{L\\'}\\\\right)\\n\\\\end{align}\\nThe labels of image tokens come from an image tokenizer, which can transform dense image pixels into discrete tokens according to a visual vocabulary~\\\\cite{ramesh2021zero}.\\nThus MIM facilitates learning high-level layout structures rather than noisy low-level details.\\n\\n\\\\noindent \\\\textbf{Objective III: Word-Patch Alignment (WPA).}\\nFor documents, each text word corresponds to an image patch. As we randomly mask text and image tokens with MLM and MIM respectively, there is no explicit alignment learning between text and image modalities.\\nWe thus propose a WPA objective to learn a fine-grained alignment between text words and image patches.\\nThe WPA objective is to predict whether the corresponding image patches of a text word are masked.\\nSpecifically, we assign an \\\\emph{aligned} label to an \\\\emph{unmasked} text token when its corresponding image tokens are also unmasked.\\nOtherwise, we assign an \\\\emph{unaligned} label.\\nWe exclude the \\\\emph{masked} text tokens when calculating WPA loss to prevent the model from learning a correspondence between masked text words and image patches.\\nWe use a two-layer MLP head that inputs contextual text and image and outputs the binary {aligned}/{unaligned} labels with a binary cross-entropy loss:\\n\\\\begin{align}\\nL_{WPA}\\\\left(\\\\theta\\\\right) &= -\\\\sum_{\\\\ell=1}^{L-L\\'} \\\\log p_{\\\\theta}\\\\left(\\\\vect{z}_\\\\ell \\\\mid \\\\mat{X}^{M\\'}, \\\\mat{Y}^{L\\'}\\\\right),\\n\\\\end{align}\\nwhere $L-L\\'$ is the number of unmasked text tokens, $\\\\vect{z}_\\\\ell$ is the binary label of language token in the $\\\\ell$ position.\\n\\n\\\\begin{table*}[t]\\n    \\\\centering\\n    % \\\\small\\n    \\\\caption{\\\\textbf{Comparison with existing published models} on the CORD \\\\protect\\\\cite{park2019cord}, FUNSD \\\\protect\\\\cite{jaume2019funsd}, RVL-CDIP \\\\protect\\\\cite{harley2015icdar}, and DocVQA \\\\protect\\\\cite{mathew2021docvqa} datasets.\\n    ``T/L/I\\'\\' denotes ``text/layout/image\\'\\' modality. ``R/G/P\\'\\' denotes ``region/grid/patch\\'\\' image embedding.\\n    We multiply all values by a hundred for better readability.\\n    $^\\\\dagger$In the UDoc paper \\\\protect\\\\cite{gu2021unidoc}, the CORD splits are 626/247 receipts for training/test instead of the official 800/100 training/test receipts adopted by other works. Thus the score$^\\\\dagger$ is not directly comparable to other scores.\\n    Models denoted with $^\\\\ddagger$ use more data to train DocVQA and are expected to score higher. For example, TILT introduces one more supervised training stage on more QA datasets \\\\protect\\\\cite{Powalski2021GoingFB}. StructuralLM additionally uses the validation set in training \\\\protect\\\\cite{Li2021StructuralLMSP}.\\n    }\\n    \\\\label{tab:sota}\\n    \\\\begin{tabular}{llllcccc}\\n    \\\\toprule\\n    \\\\multirow{2}{*}{\\\\bf Model} & \\\\multirow{2}{*}{\\\\bf Parameters} & \\\\multirow{2}{*}{\\\\bf Modality} & \\\\multirow{2}{*}{\\\\bf Image Embedding} & \\\\bf FUNSD & \\\\bf CORD & \\\\bf RVL-CDIP  & \\\\bf DocVQA  \\\\\\\\\\n     & & & & \\\\bf F1$\\\\uparrow$  & \\\\bf F1$\\\\uparrow$ & \\\\bf Accuracy$\\\\uparrow$ & \\\\bf ANLS$\\\\uparrow$ \\\\\\\\\\n     \\\\midrule\\n     $\\\\textrm{BERT}_{\\\\rm BASE}$~\\\\cite{devlin2019bert} & 110M & T & None & 60.26 & 89.68 &  89.81 & 63.72\\\\\\\\\\n     $\\\\textrm{RoBERTa}_{\\\\rm BASE}$~\\\\cite{liu2019roberta} & 125M & T & None & 66.48 & 93.54 &  90.06 & 66.42\\\\\\\\\\n     $\\\\textrm{BROS}_{\\\\rm BASE}$~\\\\cite{hong2022bros} & 110M & T+L & None & 83.05 & 95.73 &  - & -\\\\\\\\\\n     $\\\\textrm{LiLT}_{\\\\rm BASE}$~\\\\cite{wang2022LiLT} & - & T+L & None & 88.41 & 96.07 & 95.68* & - \\\\\\\\\\n     $\\\\textrm{LayoutLM}_{\\\\rm BASE}$~\\\\cite{xu2020layoutlm} & 160M & T+L+I (R) & ResNet-101 (fine-tune) & 79.27 & - & 94.42 & -\\\\\\\\\\n     $\\\\textrm{SelfDoc}$~\\\\cite{li2021selfdoc} & - & T+L+I (R) & ResNeXt-101 & 83.36 & - & 92.81 & -\\\\\\\\\\n     $\\\\textrm{UDoc}$~\\\\cite{gu2021unidoc} & 272M & T+L+I (R) & ResNet-50 & 87.93 & \\\\demph{98.94}$^\\\\dagger$  & 95.05 & -\\\\\\\\\\n     $\\\\textrm{TILT}_{\\\\rm BASE}$~\\\\cite{Powalski2021GoingFB} & 230M & T+L+I (R) & U-Net & - & 95.11 & 95.25 & \\\\demph{83.92}$^\\\\ddagger$\\\\\\\\\\n     $\\\\textrm{XYLayoutLM}_{\\\\rm BASE}$~\\\\cite{gu2022xylayoutlm} & - & T+L+I (G) & ResNeXt-101 & 83.35 & - & - & -\\\\\\\\\\n     $\\\\textrm{LayoutLMv2}_{\\\\rm BASE}$~\\\\cite{xu-etal-2021-layoutlmv2} & 200M & T+L+I (G) & ResNeXt101-FPN & 82.76 & 94.95 & 95.25 & 78.08\\\\\\\\\\n     $\\\\textrm{DocFormer}_{\\\\rm BASE}$~\\\\cite{Appalaraju_2021_ICCV} & 183M & T+L+I (G) & ResNet-50 & 83.34 & 96.33 & \\\\textbf{96.17} & - \\\\\\\\\\n     \\\\bf $\\\\textrm{LayoutLMv3}_{\\\\rm BASE}$ (Ours) & 133M & T+L+I (P) & Linear & \\\\textbf{90.29} & \\\\textbf{96.56} & 95.44 & \\\\textbf{78.76} \\\\\\\\\\n     \\\\midrule\\n     $\\\\textrm{BERT}_{\\\\rm LARGE}$~\\\\cite{devlin2019bert} & 340M & T& None & 65.63 & 90.25 & 89.92 & 67.45\\\\\\\\\\n     $\\\\textrm{RoBERTa}_{\\\\rm LARGE}$~\\\\cite{liu2019roberta} & 355M & T& None & 70.72 & 93.80 &  90.11 & 69.52 \\\\\\\\\\n     $\\\\textrm{LayoutLM}_{\\\\rm LARGE}$~\\\\cite{xu2020layoutlm} & 343M & T+L& None & 77.89 & - & 91.90 & - \\\\\\\\\\n     $\\\\textrm{BROS}_{\\\\rm LARGE}$~\\\\cite{hong2022bros} & 340M & T+L& None & 84.52 & 97.40 &  - & -\\\\\\\\\\n     $\\\\textrm{StructuralLM}_{\\\\rm LARGE}$~\\\\cite{Li2021StructuralLMSP} & 355M & T+L& None & 85.14 & - & \\\\textbf{96.08} & \\\\demph{83.94}$^\\\\ddagger$ \\\\\\\\\\n     $\\\\textrm{FormNet}$~\\\\cite{lee2022formnet} & 217M & T+L & None & 84.69 & - & - & - \\\\\\\\\\n     $\\\\textrm{FormNet}$~\\\\cite{lee2022formnet} & 345M & T+L & None & - & 97.28 & - & - \\\\\\\\\\n     $\\\\textrm{TILT}_{\\\\rm LARGE}$~\\\\cite{Powalski2021GoingFB} & 780M & T+L+I (R) & U-Net & - & 96.33 & 95.52 & \\\\demph{87.05}$^\\\\ddagger$\\\\\\\\\\n     $\\\\textrm{LayoutLMv2}_{\\\\rm LARGE}$~\\\\cite{xu-etal-2021-layoutlmv2} & 426M & T+L+I (G) & ResNeXt101-FPN & 84.20 & 96.01 & 95.64 & \\\\textbf{83.48} \\\\\\\\\\n     $\\\\textrm{DocFormer}_{\\\\rm LARGE}$~\\\\cite{Appalaraju_2021_ICCV} & 536M & T+L+I (G) & ResNet-50 & 84.55 & 96.99 & 95.50 & - \\\\\\\\\\n     \\\\bf $\\\\textrm{LayoutLMv3}_{\\\\rm LARGE}$ (Ours) & 368M & T+L+I (P) & Linear & \\\\textbf{92.08} & \\\\textbf{97.46} & {95.93} & 83.37 \\\\\\\\\\n     \\\\bottomrule\\n    \\\\multicolumn{7}{l}{\\\\footnotesize \\n    * LiLT uses image features with ResNeXt101-FPN backbone in fine-tuning RVL-CDIP.\\n    }\\n    \\\\end{tabular}\\n\\\\end{table*}\\n\\n\\n\\\\section{Experiments}\\n\\n\\\\subsection{Model Configurations}\\nThe network architecture of LayoutLMv3 follows that of LayoutLM~\\\\cite{xu2020layoutlm} and LayoutLMv2~\\\\cite{xu-etal-2021-layoutlmv2} for a fair comparison.\\nWe use base and large model sizes for LayoutLMv3.\\n$\\\\mathrm{LayoutLMv3_{BASE}}$ adopts a 12-layer Transformer encoder with 12-head self-attention, hidden size of $D=768$, and 3,072 intermediate size of feed-forward networks.\\n$\\\\mathrm{LayoutLMv3_{LARGE}}$ adopts a 24-layer Transformer encoder with 16-head self-attention, hidden size of $D=1,024$, and 4,096 intermediate size of feed-forward networks.\\nTo pre-process the text input, we tokenize the text sequence with Byte-Pair Encoding (BPE)~\\\\cite{sennrich2016neural} with a maximum sequence length $L=512$.\\nWe add a [CLS] and a [SEP] token at the beginning and end of each text sequence. When the length of the text sequence is shorter than $L$, we append [PAD] tokens to it. The bounding box coordinates of these special tokens are all zeros.\\nThe parameters for image embedding are $C\\\\times H\\\\times W = 3\\\\times 224 \\\\times 224$, $P=16$, $M=196$.\\n\\nWe adopt distributed and mixed-precision training to reduce memory costs and speed up training procedures.\\nWe also use a gradient accumulation mechanism to split the batch of samples into several mini-batches to overcome memory constraints for large batch sizes.\\nWe further use a gradient checkpointing technique for document layout analysis to reduce memory costs.\\nTo stabilize training, we follow CogView~\\\\cite{ding2021cogview} to change the computation of attention to $\\\\textrm{softmax}\\\\left(\\\\frac{\\\\mat{Q}^T\\\\mat{K}}{\\\\sqrt{d}}\\\\right)  =\\\\textrm{softmax}\\\\left(\\\\left(\\\\frac{\\\\mat{Q}^T}{\\\\alpha\\\\sqrt{d}}\\\\mat{K} - \\\\max\\\\left(\\\\frac{\\\\mat{Q}^T}{\\\\alpha\\\\sqrt{d}}\\\\mat{K}\\\\right)\\\\right)\\\\times \\\\alpha\\\\right)$, where $\\\\alpha$ is 32.\\n\\n\\n\\\\subsection{Pre-training LayoutLMv3}\\nTo learn a universal representation for various document tasks, we pre-train LayoutLMv3 on a large IIT-CDIP dataset. \\nThe \\\\textbf{IIT-CDIP Test Collection 1.0} is a large-scale scanned document image dataset, which contains about 11 million document images and can split into 42 million pages~\\\\cite{Lewis:2006:BTC:1148170.1148307}.\\nWe only use 11 million of them to train LayoutLMv3.\\nWe do not do image augmentation following LayoutLM models~\\\\cite{xu2020layoutlm, xu-etal-2021-layoutlmv2}. \\nFor the multimodal Transformer encoder along with the text embedding layer, LayoutLMv3 is initialized from the pre-trained weights of RoBERTa~\\\\cite{liu2019roberta}.\\nOur image tokenizer is initialized from a pre-trained image tokenizer in DiT, a self-supervised pre-trained document image Transformer model~\\\\cite{li2022dit}.\\nThe vocabulary size of image tokens is 8,192.\\nWe randomly initialized the rest model parameters.\\nWe pre-train LayoutLMv3 using Adam optimizer~\\\\cite{kingma2014adam} with a batch size of 2,048 for 500,000 steps.\\nWe use a weight decay of $1e-2$, and ($\\\\beta$1, $\\\\beta$2) = (0.9, 0.98).\\nFor the $\\\\mathrm{LayoutLMv3_{BASE}}$ model, we use a learning rate of $1e-4$, and we linearly warm up the learning rate over the first 4.8\\\\% steps.\\nFor $\\\\mathrm{LayoutLMv3_{LARGE}}$, the learning rate and warm-up ratio are $5e-5$ and 10\\\\%, respectively.\\n\\n\\n\\n\\\\subsection{Fine-tuning on Multimodal Tasks}\\nWe compare LayoutLMv3 with typical self-supervised pre-training approaches and categorize them by their pre-training modalities.\\n\\\\begin{itemize}\\n    \\\\item \\\\textbf{[T] text modality}: BERT~\\\\cite{devlin2019bert} and RoBERTa~\\\\cite{liu2019roberta} are typical pre-trained language models which only use text information with Transformer architecture. \\n    We use FUNSD and RVL-CDIP results of the RoBERTa from LayoutLM~\\\\cite{xu2020layoutlm} and results of BERT from LayoutLMv2~\\\\cite{xu-etal-2021-layoutlmv2}. We reproduce and report the CORD and DocVQA results of the RoBERTa.\\n    \\\\item \\\\textbf{[T+L] text and layout modalities}: LayoutLM incorporates layout information by adding word-level spatial embeddings to embeddings of BERT~\\\\cite{xu2020layoutlm}. StructuralLM leverages segment-level layout information~\\\\cite{Li2021StructuralLMSP}. BROS encodes relative layout positions~\\\\cite{hong2022bros}. LILT fine-tunes on different languages with pre-trained textual models~\\\\cite{wang2022LiLT}.\\n    FormNet leverages the spatial relationship between tokens in a form~\\\\cite{lee2022formnet}.\\n    \\\\item \\\\textbf{[T+L+I (R)] text, layout and image modalities with Faster R-CNN region features}: \\n\\tThis line of works extract image region features from RoI heads in the Faster R-CNN model~\\\\cite{Ren2015FasterRT}. Among them, LayoutLM~\\\\cite{xu2020layoutlm} and TILT~\\\\cite{Powalski2021GoingFB} use OCR words\\' bounding box to serve as region proposals and add the region features to corresponding text embeddings.\\n\\tSelfDoc~\\\\cite{li2021selfdoc} and UDoc~\\\\cite{gu2021unidoc} use document object proposals and concatenate region features with text embeddings.\\n    \\\\item \\\\textbf{[T+L+I (G)] text, layout and image modalities with CNN grid features}: LayoutLMv2~\\\\cite{xu-etal-2021-layoutlmv2} and DocFormer~\\\\cite{Appalaraju_2021_ICCV} extract image grid features with a CNN backbone without object detection. XYLayoutLM~\\\\cite{gu2022xylayoutlm} adopts the architecture of LayoutLMv2 and improves layout representation.\\n    \\\\item \\\\textbf{[T+L+I (P)] text, layout, and image modalities with linear patch features}: LayoutLMv3 replaces CNN backbones with simple linear embedding to encode image patches.\\n\\\\end{itemize}\\n\\n\\n\\\\begin{table*}[t]\\n    \\\\centering\\n    % \\\\small\\n    \\\\caption{\\\\textbf{Document layout analysis} mAP @ IOU [0.50:0.95] on PubLayNet validation set. All models use only information from the vision modality.\\n    LayoutLMv3 outperforms the compared ResNets \\\\protect\\\\cite{zhong2019publaynet, gu2021unidoc} and vision Transformer \\\\protect\\\\cite{li2022dit} backbones.\\n    }\\n    \\\\label{tab:publaynet}\\n    \\\\begin{tabular}{ccccccccc}\\n        \\\\toprule\\n        \\\\multicolumn{1}{c}{\\\\bf Model} & \\\\bf Framework & \\\\bf Backbone & \\\\bf Text & \\\\bf Title & \\\\bf List & \\\\bf Table & \\\\bf Figure & \\\\bf Overall \\\\\\\\\\n        \\\\midrule\\n        PubLayNet\\\\cite{zhong2019publaynet} & Mask R-CNN & ResNet-101 & 91.6 & 84.0 & 88.6 & 96.0 & 94.9 & 91.0\\\\\\\\\\n        $\\\\textrm{DiT}_{\\\\rm BASE}$~\\\\cite{li2022dit} & Mask R-CNN & Transformer & 93.4 & 87.1 & 92.9 & 97.3 & 96.7 & 93.5\\\\\\\\\\n        UDoc~\\\\cite{gu2021unidoc} & Faster R-CNN & ResNet-50 & 93.9 & 88.5 & 93.7 & 97.3 & 96.4 & 93.9 \\\\\\\\\\n        $\\\\textrm{DiT}_{\\\\rm BASE}$~\\\\cite{li2022dit} & Cascade R-CNN & Transformer & 94.4 & 88.9 & 94.8 & 97.6 & 96.9 & 94.5\\\\\\\\\\n        \\\\midrule\\n        \\\\bf $\\\\textrm{LayoutLMv3}_{\\\\rm BASE}$ (Ours) & Cascade R-CNN & Transformer & \\\\bf 94.5 & \\\\bf 90.6 & \\\\bf 95.5 & \\\\bf 97.9 & \\\\bf 97.0 & \\\\bf 95.1 \\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n\\\\end{table*}\\n\\n\\nWe fine-tune LayoutLMv3 on multimodal tasks on publicly available benchmarks.\\nResults are shown in Table~\\\\ref{tab:sota}.\\n\\n\\\\noindent \\\\textbf{Task I: Form and Receipt Understanding.}\\nForm and receipt understanding tasks require extracting and structuring forms and receipts\\' textual content.\\nThe tasks are a sequence labeling problem aiming to tag each word with a label.\\nWe predict the label of the last hidden state of each text token with a linear layer and an MLP classifier for form and receipt understanding tasks, respectively. \\n\\nWe conduct experiments on the FUNSD dataset and the CORD dataset.\\nThe \\\\textbf{FUNSD}~\\\\cite{jaume2019funsd} is a noisy scanned form understanding dataset sampled from the RVL-CDIP dataset~\\\\cite{harley2015icdar}.\\nThe FUNSD dataset contains 199 documents with comprehensive annotations for 9,707 semantic entities. We focus on the semantic entity labeling task on the FUNSD dataset to assign each semantic entity a label among ``question\\'\\', ``answer\\'\\', ``header\\'\\' or ``other\\'\\'.\\nThe training and test splits contain 149 and 50 samples, respectively.\\n\\\\textbf{CORD}~\\\\cite{park2019cord} is a receipt key information extraction dataset with 30 semantic labels defined under 4 categories.\\nIt contains 1,000 receipts of 800 training, 100 validation, and 100 test examples. \\nWe use officially-provided images and OCR annotations.\\nWe fine-tune LayoutLMv3 for 1,000 steps with a learning rate of $1e-5$ and a batch size of 16 for FUNSD, and $5e-5$ and 64 for CORD.\\n\\nWe report F1 scores for this task.\\nFor the large model size, the LayoutLMv3 achieves an F1 score of 92.08 on the FUNSD dataset, which significantly outperforms the SOTA result of 85.14 provided by StructuralLM~\\\\cite{Li2021StructuralLMSP}.\\nNote that LayoutLMv3 and StructuralLM use segment-level layout positions, while the other works use word-level layout positions. Using segment-level positions may benefit the semantic entity labeling task on FUNSD~\\\\cite{Li2021StructuralLMSP}, so the two types of work are not directly comparable.\\nThe LayoutLMv3 also achieves SOTA F1 scores on the CORD dataset for both base and large model sizes.\\nThe results show that LayoutLMv3 can significantly benefit the text-centric form and receipt understanding tasks.\\n\\n\\n\\\\noindent \\\\textbf{Task II: Document Image Classification.}\\nThe document image classification task aims to predict the category of document images.\\nWe feed the output hidden state of the special classification token ([CLS]) into an MLP classifier to predict the class labels.\\n\\nWe conduct experiments on the \\\\textbf{RVL-CDIP} dataset.\\nIt is a subset of the IIT-CDIP collection labeled with 16 categories~\\\\cite{harley2015icdar}. \\nRVL-CDIP dataset contains 400,000 document images, among them 320,000 are training images, 40,000 are validation images, and 40,000 are test images.\\nWe extract text and layout information using Microsoft Read API.\\nWe fine-tune LayoutLMv3 for 20,000 steps with a batch size of 64 and a learning rate of $2e-5$.\\n\\n\\nThe evaluation metric is the overall classification accuracy.\\nLayoutLMv3 achieves better or comparable results with a much smaller model size than previous works.\\nFor example, compared to LayoutLMv2, LayoutLMv3 achieves an absolute improvement of 0.19\\\\% and 0.29\\\\% in the base model and large model size, respectively, with a much simpler image embedding (i.e., Linear vs. ResNeXt101-FPN).\\nThe results show that our simple image embeddings can achieve desirable results on image-centric tasks.\\n\\n\\n\\\\noindent \\\\textbf{Task III: Document Visual Question Answering.}\\nDocument visual question answering requires a model to take a document image and a question as input and output an answer~\\\\cite{mathew2021docvqa}.\\nWe formalize this task as an extractive QA problem, where the model predicts start and end positions by classifying the last hidden state of each text token with a binary classifier.\\n\\nWe conduct experiments on the \\\\textbf{DocVQA} dataset, a standard dataset for visual question answering on document images~\\\\cite{mathew2021docvqa}. \\nThe official partition of the DocVQA dataset consists of 10,194/1,286/1,287 images and 39,463/5,349/5,188 questions for training/validation/test set, respectively. We train our model on the training set, evaluate the model on the test set, and report results by submitting them to the official evaluation website.\\nWe use Microsoft Read API to extract text and bounding boxes from images and use heuristics to find given answers in the extracted text as in LayoutLMv2.\\nWe fine-tune $\\\\textrm{LayoutLMv3}_{\\\\rm BASE}$ for 100,000 steps with a batch size of 128, a learning rate of $3e-5$, and a warmup ratio of 0.048.\\nFor $\\\\textrm{LayoutLMv3}_{\\\\rm LARGE}$, the step size, batch size, learning rate and warmup ratio are 200,000, 32, $1e-5$, and 0.1, respectively.\\n\\nWe report the commonly-used edit distance-based metric ANLS (also known as Average Normalized Levenshtein Similarity).\\nThe $\\\\textrm{LayoutLMv3}_{\\\\rm BASE}$ improves the ANLS score of $\\\\textrm{LayoutLMv2}_{\\\\rm BASE}$ from 78.08 to 78.76, with much simpler image embedding (i.e., from ResNeXt101-FPN to Linear embedding).\\nThe $\\\\textrm{LayoutLMv3}_{\\\\rm LARGE}$ further gains an absolute ANLS score of 4.61 over $\\\\textrm{LayoutLMv3}_{\\\\rm BASE}$.\\nThe results show that LayoutLMv3 is effective for the document visual question answering task.\\n\\n\\n\\n\\\\begin{table*}[t]\\n    \\\\centering\\n    \\\\caption{\\\\textbf{Ablation study on image embeddings and pre-training objectives} on typical text-centric tasks (form and receipt understanding on FUNSD and CORD) and image-centric tasks (document image classification on RVL-CDIP and document layout analysis on PubLayNet).\\n    All models were trained at $\\\\mathrm{BASE}$ size on 1 million data for 150,000 steps with learning rate $3e-4$.\\n    % for experimental efficiency.\\n    }\\n    \\\\label{tab:ablation}\\n    \\\\begin{tabular}{ccclcccc}\\n    \\\\toprule\\n    \\\\multirow{2}{*}{\\\\bf \\\\#} & \\\\bf Image & \\\\multirow{2}{*}{\\\\bf Parameters} & \\\\bf Pre-training & \\\\bf FUNSD & \\\\bf CORD & \\\\bf RVL-CDIP  & \\\\bf PubLayNet \\\\\\\\\\n     & \\\\bf Embed & & \\\\bf Objective(s) & F1$\\\\uparrow$ & F1$\\\\uparrow$ & Accuracy$\\\\uparrow$ & MAP$\\\\uparrow$ \\\\\\\\\\n    \\\\midrule\\n    1 & None & 125M & MLM & {88.64} & {96.27} & 95.33 & Not Applicable \\\\\\\\\\n    2 & Linear & 126M & MLM & 89.39 & 96.11 & 95.00 & Loss Divergence \\\\\\\\\\n    3 & Linear & 132M & MLM+MIM  & 89.19 & 96.30 & {95.42}& {94.38} \\\\\\\\\\n    4 & Linear & 133M & MLM+MIM+WPA & \\\\textbf{89.78} &  \\\\textbf{96.49} & \\\\textbf{95.53} & \\\\textbf{94.43} \\\\\\\\\\n    \\\\bottomrule\\n    \\\\end{tabular}\\n\\\\end{table*}\\n\\n\\\\subsection{Fine-tuning on a Vision Task}\\nTo demonstrate the generality of LayoutLMv3 from the multimodal domain to the visual domain, we transfer LayoutLMv3 to a \\\\textbf{document layout analysis} task.\\nThis task is about detecting the layouts of unstructured digital documents by providing bounding boxes and categories such as tables, figures, texts, etc.\\nThis task helps parse the documents into a machine-readable format for downstream applications.\\nWe model this task as an object detection problem without text embedding, which is effective in existing works~\\\\cite{zhong2019publaynet, gu2021unidoc, li2022dit}.\\nWe integrate the LayoutLMv3 as feature backbone in the Cascade R-CNN detector~\\\\cite{cai2018cascade} with FPN~\\\\cite{lin2017feature} implemented using the Detectron2~\\\\cite{wu2019detectron2}.\\nWe adopt the standard practice to extract single-scale features from different Transformer layers, such as layers 4, 6, 8, and 12 of the LayoutLMv3 base model. We use resolution-modifying modules to convert the single-scale features into the multiscale FPN features~\\\\cite{ali2021xcit,li2021benchmarking,li2022dit}.\\n\\nWe conduct experiments on \\\\textbf{PubLayNet} dataset~\\\\cite{zhong2019publaynet}.\\nThe dataset contains research paper images annotated with bounding boxes and polygonal segmentation across five document layout categories: text, title, list, figure, and table.\\nThe official splits contain 335,703 training images, 11,245 validation images, and 11,405 test images. We train our model on the training split and evaluate our model on the validation split following standard practice~\\\\cite{zhong2019publaynet,gu2021unidoc,li2022dit}.\\nWe train our model for 60,000 steps using the AdamW optimizer with 1,000 warm-up steps and a weight decay of 0.05 following DiT~\\\\cite{li2022dit}. Since LayoutLMv3 is pre-trained with inputs from both vision and language modalities, we use a larger batch size of 32 and a lower learning rate of $2e-4$ empirically.\\nWe do not use flipping or cropping augmentation strategy in the fine-tuning stage to be consistent with our pre-training stage.\\nWe do not use relative positions in self-attention networks as DiT.\\n\\nWe measure the performance using the mean average precision (MAP) @ intersection over union (IOU) [0.50:0.95] of bounding boxes and report results in Table~\\\\ref{tab:publaynet}.\\nWe compare with the ResNets~\\\\cite{zhong2019publaynet, gu2021unidoc} and the concurrent vision Transformer~\\\\cite{li2022dit} backbones.\\nLayoutLMv3 outperforms the other models in all metrics, achieving an overall mAP score of 95.1.\\nLayoutLMv3 achieves a high gain in the ``Title\\'\\' category. Since titles are typically much smaller than other categories and can be identified by their textual content, we attribute this improvement to our incorporation of language modality in pre-training LayoutLMv3.\\nThese results demonstrate the generality and superiority of LayoutLMv3.\\n\\n\\\\begin{figure}[t]\\n    \\\\centering\\n    \\\\includegraphics[width=\\\\linewidth]{figures/loss.png}\\n    \\\\caption{Loss convergence curves of fine-tuning the ablated models of  LayoutLMv3 on PubLayNet dataset.\\n    The loss of model \\\\#2 did not converge.\\n    By incorporating the MIM objective, the loss converges normally.\\n    The WPA objective further decreases the loss.\\n    Best viewed in color.\\n    }\\n    \\\\label{fig:loss}\\n\\\\end{figure} \\n\\n\\\\subsection{Ablation Study}\\\\label{subsec:ablation_study}\\nIn Table~\\\\ref{tab:ablation} we study the effect of our image embeddings and pre-training objectives.\\nWe first build a baseline model \\\\#1 that uses text and layout information, pre-trained with MLM objective.\\nThen we use linearly projected image patches as the image embedding of the baseline model, denoted as model \\\\#2.\\nWe further pre-train model \\\\#2 with MIM and WPA objectives step by step and denote the new models as \\\\#3 and \\\\#4, respectively.\\n\\n\\nIn Figure~\\\\ref{fig:loss}, we visualize losses of models \\\\#2, \\\\#3, and \\\\#4 when fine-tuned on the PubLayNet dataset with a batch size of 16 and a learning rate of $2e-4$.\\nWe have tried to train the model \\\\#2 with learning rates of \\\\{$1e-4$, $2e-4$, $4e-4$\\\\} combined with batch sizes of \\\\{$16$, $32$\\\\}, but the loss of model \\\\#2 did not converge and the mAP score on PubLayNet is near zero.\\n\\n\\\\noindent \\\\textbf{Effect of Linear Image Embedding.}\\nWe observe that model \\\\#1 without image embedding has achieved good results on some tasks.\\nThis suggests that language modality, including text and layout information, plays a vital role in document understanding.\\nHowever, the results are still unsatisfactory.\\nMoreover, model \\\\#1 cannot conduct some image-centric document analysis tasks without vision modality.\\nFor example, the vision modality is critical for the document layout analysis task on PubLayNet because bounding boxes are tightly integrated with images.\\nOur simple design of linear image embedding combined with appropriate pre-training objectives can consistently improve not only image-centric tasks, but also some text-centric tasks further.\\n\\n\\n\\\\noindent \\\\textbf{Effect of MIM pre-training objective.}\\nSimply concatenating linear image embedding with text embedding as input to model \\\\#2 deteriorates performance on CORD and RVL-CDIP, while the loss on PubLayNet diverges.\\nWe speculate that the model failed to learn meaningful visual representation on the linear patch embeddings without any pre-training objective associated with image modality.\\nThe MIM objective mitigates this problem by preserving the image information until the last layer of the model by randomly masking out a portion of input image patches and reconstructing them in the output~\\\\cite{kim2021vilt}.\\nComparing the results of model \\\\#3 and model \\\\#2, the MIM objective benefits CORD and RVL-CDIP.\\nAs simply using linear image embedding has improved FUNSD, MIM does not further contribute to FUNSD.\\nBy incorporating the MIM objective in training, the loss converges when fine-tuning PubLayNet as shown in Figure~\\\\ref{fig:loss}, and we obtain a desirable mAP score.\\nThe results indicate that MIM can help regularize the training. Thus MIM is critical for vision tasks like document layout analysis on PubLayNet.\\n\\n\\n\\\\noindent \\\\textbf{Effect of WPA pre-training objective.}\\nBy comparing models \\\\#3 and \\\\#4 in Table~\\\\ref{tab:ablation}, we observe that the WPA objective consistently improves all tasks.\\nMoreover, the WPA objective decreases the loss of the vision task on PubLayNet in Figure~\\\\ref{fig:loss}.\\nThese results confirm the effectiveness of WPA not only in cross-modal representation learning, but also in image representation learning.\\n\\n\\\\noindent \\\\textbf{Parameter Comparisons.}\\nThe table shows that incorporating image embedding for a $16 \\\\times 16$ patch projection (\\\\#1 $\\\\rightarrow$ \\\\#2) introduces only 0.6M parameters.\\nThe parameters are negligible compared to the parameters of CNN backbones (e.g., 44M for ResNet-101).\\nA MIM head and a WPA head introduce 6.9M and 0.6M parameters in the pre-training stage.\\nThe parameter overhead introduced by image embedding is marginal compared to the MLM head,  which has 39.2M parameters for a text vocabulary size of 50,265.\\nWe did not take count of the image tokenizer when calculating parameters as the tokenizer is a standalone module for generating the labels of MIM but is not integrated into the Transformer backbone.\\n\\n\\n\\\\section{Related Work}\\n\\n\\\\textbf{Multimodal self-supervised pre-training} technique has made a rapid progress in \\\\emph{document intelligence} due to its successful applications of document layout and image representation learning~\\\\cite{xu2020layoutlm,xu-etal-2021-layoutlmv2,xu2021layoutxlm,pramanik2020towards,garncarek2021lambert,hong2022bros,Powalski2021GoingFB,wu2021lampret,Li2021StructuralLMSP,li2021selfdoc,Appalaraju_2021_ICCV,li2021structext,gu2021unidoc,wang2022LiLT,gu2022xylayoutlm,lee2022formnet}.\\nLayoutLM and following works joint layout representation learning by encoding spatial coordinates of text~\\\\cite{xu2020layoutlm,Li2021StructuralLMSP,hong2022bros,lee2022formnet}. \\nVarious works then joint image representation learning by combining CNNs with Transformer~\\\\cite{vaswani2017attention} self-attention networks.\\nThese works either extract CNN grid features~\\\\cite{xu-etal-2021-layoutlmv2,Appalaraju_2021_ICCV} or rely on an object detector to extract region features~\\\\cite{xu2020layoutlm,Powalski2021GoingFB,li2021selfdoc,gu2021unidoc}, which accounts for heavy computation bottleneck or requires region supervision.\\nIn the field of \\\\emph{natural images vision-and-language pre-training} (VLP), research works have seen a shift from region features~\\\\cite{tan2019lxmert,su2019vl,chen2020uniter} to grid features~\\\\cite{huang2021seeing} to lift limitations of pre-defined object classes and region supervision.\\nInspired by vision Transformer (ViT)~\\\\cite{dosovitskiy2020vit}, there have also been recent efforts in VLP without CNNs to overcome the weakness of CNN. Still, most rely on separate self-attention networks to learn visual features; thus, their computational cost is not reduced~\\\\cite{xue2021probing,li2021align,dou2021empirical}.\\nAn exception is ViLT, which learns visual features with a lightweight linear layer and significantly cuts down the model size and running time~\\\\cite{kim2021vilt}.\\nInspired by ViLT, our LayoutLMv3 is the first multimodal model in Document AI that utilizes image embeddings without CNNs.\\n\\n\\\\textbf{Reconstructive pre-training objectives} revolutionized representation learning.\\nIn \\\\emph{NLP} research, BERT firstly proposed ``masked language modeling\\'\\' (MLM) to learn bidirectional representations and advanced the state of the arts on broad language understanding tasks~\\\\cite{devlin2019bert}.\\nIn the field of \\\\emph{CV}, Masked Image Modeling (MIM) aims to learn rich visual representations via predicting masked content conditioning in visible context.\\nFor example, ViT reconstructs the mean color of masked patches, which leads to performance gains in ImageNet classification~\\\\cite{dosovitskiy2020vit}.\\nBEiT reconstructs visual tokens learned by a discrete VAE, achieving competitive results in image classification and semantic segmentation~\\\\cite{bao2022beit}.\\nDiT extends BEiT to document images to document layout analysis~\\\\cite{li2022dit}.\\n\\nInspired by MLM and MIM, researchers in the field of \\\\emph{vision-and-language} have explored \\\\textbf{reconstructive objectives for multimodal representation learning}.\\nWhereas most well-performing vision-and-language pre-training (VLP) models use the MLM proposed by BERT on text modality, they differ in their pre-training objectives for the image modality.\\nThere are three variants of MIM corresponding to different image embeddings: masked region modeling (MRM), masked grid modeling (MGM), and masked patch modeling (MPM).\\nMRM has been proven to be effective in regressing original region features~\\\\cite{tan2019lxmert,chen2020uniter,li2021selfdoc} or classifying object labels~\\\\cite{chen2020uniter,lu2019vilbert,tan2019lxmert} for masked regions.\\nMGM has also been explored in the SOHO, whose objective is to predict the mapping index in a visual dictionary for masked grid features~\\\\cite{huang2021seeing}.\\nFor patch-level image embedding, Visual Parsing~\\\\cite{xue2021probing} proposed to mask visual tokens according to the attention weights in their self-attention image encoder, which does not apply to simple linear image encoders.\\nViLT~\\\\cite{kim2021vilt} and METER~\\\\cite{dou2021empirical} attempt to leverage MPM similar to ViT~\\\\cite{dosovitskiy2020vit} and BEiT~\\\\cite{bao2022beit}, which respectively reconstruct the mean color and discrete tokens in visual vocabularies for image patches, but resulted in degraded performance on downstream tasks.\\nOur LayoutLMv3 firstly demonstrates the effectiveness of MIM for linear patch image embedding.\\n\\nVarious \\\\textbf{cross-modal objectives} are further developed for vision and language (VL) alignment learning in multimodal models.\\nImage-text matching is widely used to learn a coarse-grained VL alignment~\\\\cite{chen2020uniter,huang2021seeing,kim2021vilt,xu-etal-2021-layoutlmv2,Appalaraju_2021_ICCV}.\\nTo learn a fine-grained VL alignment, UNITER proposes a word-region alignment objective based on optimal transports, which calculates the minimum cost of transporting the contextualized image embeddings to word embeddings~\\\\cite{chen2020uniter}.\\nViLT extends this objective to patch-level image embeddings~\\\\cite{kim2021vilt}.\\nUnlike natural images, document images imply an explicit fine-grained alignment relationship between text words and image areas.\\nUsing this relationship, UDoc uses contrastive learning and similarity distillation to align the image and text belonging to the same area~\\\\cite{gu2021unidoc}.\\nLayoutLMv2 covers some text lines in raw images and predicts whether each text token is covered~\\\\cite{xu-etal-2021-layoutlmv2}.\\nIn contrast, we naturally utilize the masking operations in MIM to construct aligned/unaligned pairs in an effective and unified way.\\n\\n\\\\section{Conclusion and Future Work}\\nIn this paper, we present LayoutLMv3 to pre-train the multimodal Transformer for Document AI, which redesigns the model architecture and pre-training objectives for LayoutLM. \\nDistinguishing from the existing multimodal model in Document AI, LayoutLMv3 does not rely on a pre-trained CNN or Faster R-CNN backbone to extract visual features, significantly saving parameters and eliminating region annotations.\\nWe use unified text and image masking pre-training objectives: masked language modeling, masked image modeling, and word-patch alignment, to learn multimodal representations.\\nExtensive experimental results have demonstrated the generality and superiority of LayoutLMv3 for both text-centric and image-centric Document AI tasks with the simple architecture and unified objectives.\\nIn future research, we will investigate scaling up pre-trained models\\nso that the models can leverage more training data \\nto drive SOTA results further. In addition, we will explore few-shot and zero-shot learning capabilities to facilitate more real-world business scenarios in the Document AI industry. \\n\\n\\n\\\\section{Acknowledgement}\\nWe are grateful to Yiheng Xu for fruitful discussions and inspiration.\\nThis work was supported by the NSFC (U1811461) and the Program for Guangdong Introducing Innovative and Entrepreneurial Teams under Grant NO.2016ZT06D211.\\n\\n\\n% -------------------- Appendix / Supplementary material ----------\\n\\n\\\\begin{table*}[ht]\\n    \\\\centering\\n    \\\\caption{\\\\textbf{Visual information extraction in Chinese} F1 score on the EPHOIE test set.}\\n    \\\\label{tab:EPHOIE}\\n    \\\\resizebox{\\\\textwidth}{!}\\n    {\\n    \\\\begin{tabular}{llllllllllll}\\n        \\\\toprule\\n        \\\\multicolumn{1}{l}{\\\\bf Model} & \\\\bf Subject & \\\\bf Test Time & \\\\bf Name & \\\\bf School & \\\\bf \\\\#Examination & \\\\bf \\\\#Seat & \\\\bf Class & \\\\bf \\\\#Student & \\\\bf Grade & \\\\bf Score & \\\\bf Mean \\\\\\\\\\n        \\\\midrule\\n        BiLSTM+CRF~\\\\cite{lample2016neural} & 98.51 & 100.0 & 98.87 & 98.80 & 75.86 & 72.73 & 94.04 & 84.44 & 98.18 & 69.57 & 89.10 \\\\\\\\\\n        GCN-based~\\\\cite{liu2019graph} & 98.18 & 100.0 & 99.52 & \\\\bf 100.0 & 88.17 & 86.00 & 97.39 & 80.00 & 94.44 & 81.82 & 92.55 \\\\\\\\\\n        GraphIE~\\\\cite{qian2019graph} & 94.00 & 100.0 & 95.84 & 97.06 & 82.19 & 84.44 & 93.07 & 85.33 & 94.44 & 76.19 & 90.26 \\\\\\\\\\n        TRIE~\\\\cite{zhang2020trie} & 98.79 & 100.0 & 99.46 & 99.64 & 88.64 & 85.92 & 97.94 & 84.32 & 97.02 & 80.39 & 93.21 \\\\\\\\\\n        VIES~\\\\cite{wang2021towards} & \\\\bf 99.39 & 100.0 &  99.67 & 99.28 & 91.81 & 88.73 & \\\\bf 99.29 & 89.47 & 98.35 & 86.27 & 95.23 \\\\\\\\\\n        StrucTexT~\\\\cite{li2021structext} & 99.25 & 100.0 & 99.47 & 99.83 & 97.98 & 95.43 & 98.29 & 97.33 & \\\\bf 99.25 & 93.73 & 97.95 \\\\\\\\\\n        \\\\midrule\\n        \\\\bf $\\\\textrm{LayoutLMv3-Chinese}_{\\\\rm BASE}$ (Ours) & 98.99 & \\\\bf 100.0 & \\\\bf 99.77 & 99.20 & \\\\bf 100.0 & \\\\bf 100.0 & 98.82 & \\\\bf 99.78 & 98.31 & \\\\bf 97.27 & \\\\bf 99.21 \\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n    }\\n\\\\end{table*}\\n\\n\\n\\\\newpage\\\\appendix\\n\\\\section{Appendix}\\n\\n\\\\subsection{LayoutLMv3 in Chinese}\\n\\n\\\\noindent \\\\textbf{Pre-training LayoutLMv3 in Chinese.}\\nTo demonstrate the effectiveness of LayoutLMv3 in not only English but also in the Chinese language, we pre-train a LayoutLMv3-Chinese model in base size. It is trained on 50 million document pages in Chinese.\\nWe collect large-scale Chinese documents by downloading publicly available digital-born documents and following the principles of Common Crawl (https://commoncrawl.org/) to process these documents.\\nFor the multimodal Transformer encoder along with the text embedding layer, LayoutLMv3-Chinese is initialized from the pre-trained weights of XLM-R~\\\\cite{conneau2020unsupervised}.\\nWe randomly initialized the rest model parameters.\\nOther training setting is the same as LayoutLMv3.\\n\\n\\n\\n\\\\noindent \\\\textbf{Fine-tuning on Visual Information Extraction.}\\nThe visual information extraction (VIE) requires extracting key information from document images.\\nThe task is a sequence labeling problem aiming to tag each word with a pre-defined label.\\nWe predict the label of the last hidden state of each text token with a linear layer. \\n\\nWe conduct experiments on the EPHOIE dataset.\\nThe \\\\textbf{EPHOIE}~\\\\cite{wang2021towards} is a visual information extraction dataset consisting of examination paper heads with diverse layouts and backgrounds.\\nIt contains 1,494 images with comprehensive annotations for 15,771 Chinese text instances.\\nWe focus on a token-level entity labeling task on the EPHOIE dataset to assign each character a label among ten pre-defined categories.\\nThe training and test sets contain 1,183 and 311 images, respectively.\\nWe fine-tune LayoutLMv3-Chinese for 100 epochs. The batch size is 16, and the learning rate is $5e-5$ with linear warmup over the first epoch.\\n\\nWe report F1 scores for this task and report results in Table~\\\\ref{tab:EPHOIE}.\\nThe LayoutLMv3-Chinese shows superior performance on most metrics and achieves a SOTA mean F1 score of 99.21\\\\%. The results show that LayoutLMv3 significantly benefits the VIE task in Chinese.\\n\\n\\n\\n\\n\\\\bibliographystyle{ACM-Reference-Format}\\n\\\\balance\\n\\\\bibliography{ref}\\n\\n\\\\end{document}\\n\\\\endinput'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_text = LatexNodes2Text().latex_to_text(file_content).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = Document(\n",
    "    page_content=paper_text,\n",
    "    metadata={\n",
    "        \"title\": arxiv_paper.title,\n",
    "        \"authors\": arxiv_paper.authors,\n",
    "        \"abstract\": arxiv_paper.summary,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45677"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(docs.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperlight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
